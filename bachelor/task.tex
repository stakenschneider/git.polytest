\chapter{Анализ задач и моделей глубокого обучения}

В этой главе будут подробно рассмотрены и проанализированы используемые в работе задачи и модели глубокого обучения.

\section{Анализ задач}

Задачи были подобраны так, что бы они относились к различнымм областям анализа данных. 
Далее будут описаны прикладные задачи глубокого обучения, такие как: классификация изображений, локализация объектов и прогнозирование временных рядов.


\subsection{Классификация изображений}

Задача классификации предназначена для определения того, к какой группе наиболее вероятно может быть отнесен каждый объект. 

Две основные постановки это задачи - это бинарная (относится ли изображение к данной категории) и многоклассовая (к какой категории относится изображение).

\subsubsection{Постановка задачи}

Дано множество объектов, каждый их которых принадлежит одному из нескольких классов. Надо определить, какому классу принадлежит данный экземпляр.
Каждый объект с номером $j$ можно описать вектором признаков $x_j$ .
Каждому объекту можно приписать метку класса $y_j$ .


\subsubsection{Бинарная классификация}

Дана обучающая выборка 

$$ X_m=\lbrace(x_1,y_1),...,(x_m,y_m)\rbrace (x_i,y_i)\in R^m \times Y , Y=\lbrace -1 , +1 \rbrace$$

Объекты независимы и взяты из некоторого неизвестного распределения 
$$ (x_i,y_i) \in P(x,y)$$

Цель: для всех новых значений $ x$ оценить значения $$ argmaxP(y\vert x)$$ 

\subsubsection{Многоклассовая классификация}

Дана обучающая выборка 

$$ X_m=\lbrace(x_1,y_1),...,(x_m,y_m)\rbrace (x_i,y_i)\in R^m \times Y , Y=\lbrace 1,...,K \rbrace$$ 

Объекты независимы и взяты из некоторого неизвестного распределения 
$$ (x_i,y_i) \in P(x,y)$$ 

Цель: для всех новых значений $X$ оценить значения $$ argmaxP(y\vert x)$$ 







\subsection{Локализация объекта}
При разработке систем технического зрения часто возникает задача обнаружения и локализации объекта на изображении. Она заключается в проверке наличия на входном изображении некоторого объекта и вычислении его геометрических характеристик типа положения и угловой ориентации.

\subsubsection{Постановка задачи}

Имеем набор входных изображений ${x_1,. , , , x_n} \subset \chi $ и связанных с ними аннотаций ${y_1, ..., y_n} \subset \gamma $, мы хотим получить отображение $g: \chi \mapsto \gamma $, с помощью которого мы можем автоматически маркировать новые образы.
Рассмотрим случай, когда выходное пространство состоит из метки $ \omega $, указывающей, присутствует ли объект на изображении, и вектора, указывающего верхнюю $t$, левую$l$, нижнюю $b$  и правую $ r$ границы рамки внутри изображения: $$ \gamma \equiv \lbrace ( \omega , t, l, b, r) \vert ω ∈ \lbrace + 1, -1 \rbrace, (t, l, b, r) \in R^4 \rbrace .$$ При $ \omega = -1$ координатный вектор $(t, l, b, r)$ игнорируется. Мы изучаем это сопоставление,  как $$g(x) = argmax_y f (x, y)$$, где $f (x, y)$ - дискриминантная функция, которая должна давать большое значение парам $(x, y)$, которые хорошо сопоставляются. Поэтому задача состоит в том, чтобы обучить функцию $f$, если максимизация выполняется.










\subsection{Прогнозирование временных рядов}

Прогнозирование временных рядов подразумевает, что известно значение некой функции в первых n точках временного ряда. Используя эту информацию необходимо спрогнозировать значение в $n+m$ точке временного ряда. 

\subsubsection{Постановка задачи}
Пусть значения временного ряда доступны в дискретные моменты времени $t = 1,2,...,T$. Обозначим временной ряд $Z(t) = Z(1), Z(2),...,Z(T)$. В момент времени $T$ необходимо определить значения процесса $Z(t)$ в моменты времени $T+1,...,T+P$. Момент времени $T$ называется моментом прогноза, а величина $P$ - временем упреждения.

Для того чтобы вычислить значений временного ряда в будущие моменты времени требуется определить функциональную зависимость, отражающую связь между прошлыми и будущими значениями этого ряда

$$ Z(t)=F(Z(t-1), Z(t-2), Z(t-3),...)+\varepsilon_t $$ 


Эта зависимость называется моделью прогнозирования. Требуется создать такую модель прогнозирования, для которой среднее абсолютное отклонение истинного значения от прогнозируемого стремится к минимальному для заданного $P$
$$  E=1/p \sum_{t=T+1}^{T+P} \vert \varepsilon_t \vert \rightarrow min $$
    




\section{Анализ моделей}


Из рассмотренных моделей в предыдущей главе был сделан выбор в пользу сверточной нейронной сети для задачи классификации и lstm-сети для прогнозирования временных рядов.
\subsection{Сверточная  нейронная  сеть}

В компьютерном обучении сверточная нейронная сеть представляет собой класс глубоких нейронных сетей с прямой связью, наиболее часто используемых для анализа визуальных образов, поскольку СНС вырабатывает необходимую иерархию абстрактных функций, переходя от определенных особенностей изображения к более абстрактным деталям, и далее к ещё более абстрактным деталям вплоть до выделения понятий высокого уровня. Она выделяет важные детали и опускает менее значимые.

Эти признаки, малопонятны и трудны для интерпретации настолько, что в практических системах не рекомендуется пытаться понять содержания этих признаков или пытаться их совершенствовать, вместо этого рекомендуется улучшить структуру и архитектуру сети, чтобы получить лучшие результаты.

\subsubsection{Архитектура и принцип работы}

Свое название сверточная нейронная сеть получила из-за операции свертки, хотя это название и условно. Математически это кросс-корреляция, а не свертка.

В сверточной нейронной сети ограниченная матрица весов используется в операциях свертки, они перемещаются по всему обрабатываемому слою, образуя после каждого шага сигнал активации для нейрона следующего слоя с тем же положением. Для нейронов выходного слоя используется одна и та же матрица весов, она называется ядром свертки. Он интерполируется как графическое кодирование признака. После этого следующий уровень показывает наличие признака в обработанном слое, формируя так называемую карту объектов. Ядра свертки заранее не заложены исследователем, но формируются независимо путем обучения сети методом обратного распространения ошибок. Прохождение каждого набора весов формирует собственный экземпляр карт признаков, что делает нейронную сеть многоканальной. Шаг в поиске выбирается так, чтобы не перескочить искомый признак. Операция субдискретизации уменьшает размерность карт, что ускоряет вычисление, делает сеть инвариантной к масштабу входного изображения. Сигнал проходит через ряд слоев свертки и субдискретизации. Переменные слои позволяют создавать «карты признаков». На практике это означает способность распознавать сложные иерархии признаков. На выходе слоев свертки сети дополнительно устанавливают несколько слоев полностью подключенной нейронной сети, а карты с предыдущего слоя подают на вход.

\subsubsection{Слой свертки}
Слой свёртки (convolutional layer) - включает в себя для каждого канала свой фильтр, ядро свёртки которого обрабатывает предыдущий слой по фрагментам (суммируя результаты матричного произведения для каждого фрагмента). Весовые коэффициенты ядра свёртки неизвестны и устанавливаются в процессе обучения. Особенностью свёрточного слоя является сравнительно неболь­шое количество параметров, устанавливаемое при обучении. 


\subsubsection{Слой активации}
Скалярный результат каждой свёртки попадает на функцию ак­тивации, которая представляет собой некую нелинейную функцию. Слой активации обычно логически объединяют со слоем свёртки. Функция нелинейности может быть любой по выбору исследователя, тради­ционно для этого использовали функции типа гиперболического тангенса или сигмоиды. Однако, по состоянию на 2017 год функция ReLU являются наиболее часто используемыми функциями активации в глу­боких нейросетях, в частности, в свёрточных. Она позво­лила существенно ускорить процесс обучения и одновременно упро­стить вычисления (за счёт простоты самой функции), что означа­ет блок линейной ректификации, вычисляющий функцию. 

\subsubsection{Слой субдискретизации}

Слой субдискретизации представ­ляет собой нелинейное уплотнение карты признаков, при этом группа пикселей уплотняется до одного пикселя, про­ходя нелинейное преобразование. Наиболее употребительна при этом функция максимума. Преобразования затрагивают непересекающие­ся прямоугольники или квадраты, каждый из которых ужимается в один пиксель, при этом выбирается пиксель, имеющий максимальное значение. Операция субдискретизации позволяет существенно уменьшить пространственный объём изображения. Субдискретизация интерпретируется так. Ес­ли на предыдущей операции свёртки уже были выявлены некоторые признаки, то для дальнейшей обработки настолько подробное изоб­ражение уже не нужно, и оно уплотняется до менее подробного. К тому же фильтрация уже ненужных деталей помогает не переобучать­ся. Слой субдискретизации, как правило, вставляется после слоя свёртки.

\subsubsection{Полносвязный слой}
После нескольких прохождений свёртки изображения и уплотне­ния с помощью субдискретизации система перестраивается от конкретной сетки пикселей с высоким разрешением к более абстрактным картам при­ знаков, как правило на каждом следующем слое увеличивается число каналов и уменьшается размерность изображения в каждом канале. В конце концов остаётся большой набор каналов, хранящих небольшое число данных, которые интерпретируются как самые абстрактные понятия, выявленные из исходного изображения.
Эти данные объединяются и передаются на обычную полносвяз­ную нейронную сеть, которая тоже может состоять из нескольких сло­ёв. При этом полносвязные слои уже утрачивают пространственную структуру пикселей и обладают сравнительно небольшой размерно­стью.




\subsection{Долгая краткосрочная память}
LSTM-сеть — это искусственная нейронная сеть, содержащая LSTM-модули вместо или в дополнение к другим сетевым модулям.

\subsubsection{Архитектура}
LSTM-сеть является рекуррентным сетевым модулем, который может хранить значения как для коротких, так и для длительных периодов времени. Ключом к этой функции является то, что LSTM-модуль не использует функцию активации внутри своих рекуррентных компонентов. Таким образом, хранимое значение не изменяется во времени, и градиент или штраф не исчезают при использовании об­ратного распространения ошибки во времени при тренировки сети.
Модули LSTM часто группируются в «блоки», содержащие разные модули LSTM. Такое устройство типично для «глубоких» многослойных нейронных сетей и обеспечивает параллельные вычисления.
LSTM-блоки содержат три или четыре ««вентиля»», которые используются для контроля потоков информации на входах и на вы­ходах. Этивентили реализованы как логистическая функция для расчета значений в диапазоне [0; 1]. Умножение на это значение используется для частичного разрешения или блокировки потока информации внутри и вне памяти.  «Выходной вентиль» контролирует степень, в которой значение, хранящееся в памяти, используется для вычисления функции активации выхода для блока. 

